# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C-14_jnrMKbGY5E5Mn0Zq2JliE1umiv1
"""



"""# Task
Create a Python script that summarizes lengthy articles using natural language processing techniques. The script should showcase input text and concise summaries.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install transformers torch

"""**Reasoning**:
Load the input text and preprocess it for summarization using a tokenizer from the `transformers` library.


"""

from transformers import BartTokenizer

article = """
Natural Language Processing (NLP) is a field of artificial intelligence that focuses on enabling computers to understand, interpret, and generate human language. It's a multidisciplinary field, drawing upon computer science, linguistics, and psychology. The goal of NLP is to bridge the gap between human communication and computer understanding.

One of the core challenges in NLP is the inherent ambiguity of language. Words and phrases can have multiple meanings depending on the context. For example, the word "bank" can refer to a financial institution or the side of a river. Disambiguating such instances is crucial for accurate language processing.

NLP techniques are widely used in various applications. Machine translation, like Google Translate, is a prominent example, allowing communication across language barriers. Sentiment analysis helps determine the emotional tone of text, valuable for understanding customer feedback or social media trends. Chatbots and virtual assistants, such as Siri and Alexa, rely heavily on NLP to understand user commands and provide relevant responses. Text summarization, the task at hand, aims to create concise summaries of longer documents. Information extraction focuses on identifying and extracting structured information from unstructured text.

The evolution of NLP has been significantly influenced by advancements in machine learning, particularly deep learning. Early NLP systems often relied on rule-based approaches and statistical methods. However, deep neural networks, especially transformer models, have revolutionized the field by achieving state-of-the-art results on various NLP tasks. These models can learn complex patterns and relationships in language from vast amounts of data.

Preprocessing is a crucial initial step in any NLP pipeline. This involves cleaning and preparing the raw text for analysis. Common preprocessing tasks include tokenization (breaking down text into words or sub-word units), lowercasing, removing punctuation, and handling stop words (common words like "the," "a," "is" that often don't carry much meaning). Stemming and lemmatization are techniques used to reduce words to their root form.

Tokenization is particularly important as it converts text into a sequence of tokens that can be processed by machine learning models. Different tokenizers exist, and the choice depends on the specific model being used. For transformer models, sub-word tokenization is often employed, which helps handle out-of-vocabulary words and reduces the vocabulary size.

The output of the tokenizer is typically a set of numerical IDs representing the tokens, along with attention masks and other information required by the model. This numerical representation is then fed into the summarization model to generate the summary.

In summary, NLP is a dynamic field with numerous applications, constantly evolving with advancements in machine learning. Effective preprocessing, including tokenization, is a fundamental step in building robust NLP systems for tasks like text summarization.
"""

tokenizer = BartTokenizer.from_pretrained("facebook/bart-large-cnn")

inputs = tokenizer(article, return_tensors="pt", max_length=1024, truncation=True)
print("Tokenized input keys:", inputs.keys())
print("Input IDs shape:", inputs["input_ids"].shape)

"""**Reasoning**:
Import the necessary model class and load the pre-trained model for summarization using the specified model identifier.


"""

from transformers import BartForConditionalGeneration
import torch

model = BartForConditionalGeneration.from_pretrained("facebook/bart-large-cnn")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

print("Model loaded successfully and moved to:", device)

inputs = {k: v.to(device) for k, v in inputs.items()}

summary_ids = model.generate(
    inputs["input_ids"],
    num_beams=4,
    min_length=30,
    max_length=150,
    early_stopping=True
)

summary = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]

print("Original Article:")
print(article)
print("\nGenerated Summary:")
print(summary)